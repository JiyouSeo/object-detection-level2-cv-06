{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "annotation = '/opt/ml/personel/easy_train.json' # annotation 경로"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import json\n",
    "def read_json(dir):\n",
    "    with open(dir, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return json_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import six\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data as data_\n",
    "\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter\n",
    "\n",
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"\n",
    "    from src_bbox to dst bbox using loc\n",
    "    Args:\n",
    "        src_bbox: 소스 바운딩 박스\n",
    "        loc: 델타\n",
    "    Returns: dst_bbox\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"\n",
    "    src_bbox : 예측된 좌표값(or anchor), dst_bbox: gt 좌표값 -> loc(y, x, h, w)\n",
    "    \"\"\"\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initialization\n",
    "    \"\"\"\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) \n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0) &\n",
    "        (anchor[:, 1] >= 0) &\n",
    "        (anchor[:, 2] <= H) &\n",
    "        (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "def unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret\n",
    "\n",
    "\n",
    "## util ##\n",
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "def totensor(data, cuda = True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# TrainDataset\n",
    "class TrainCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation 파일 위치\n",
    "            data_dir: data가 존재하는 폴더 경로\n",
    "            transforms : transform or not\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 어노테이션 파일 로드\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # 박스 가져오기\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # 레이블 가져오기\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
    "        boxes[:, 0] = bboxes[:, 1]\n",
    "        boxes[:, 1] = bboxes[:, 0]\n",
    "        boxes[:, 2] = bboxes[:, 3]\n",
    "        boxes[:, 3] = bboxes[:, 2]\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_dir = '/opt/ml/detection/dataset'\n",
    "dataset = TrainCustom(annotation, data_dir, transforms=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#레이블 갯수\n",
    "def get_label(dataset):\n",
    "    from collections import defaultdict\n",
    "    from tqdm import tqdm\n",
    "    idxlist = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        _, bbox, labels = dataset[i]\n",
    "        bbox, labels = bbox.tolist(), labels.tolist()\n",
    "        if len(labels) <= 5:\n",
    "            idxlist.append(i)\n",
    "\n",
    "    return idxlist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "idxs = get_label(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4883/4883 [01:11<00:00, 68.42it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "idxs[0:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 2, 3, 6, 7, 9, 10, 11, 12, 14]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def editer(annotation, idxs):\n",
    "    train_dataset = read_json(annotation) \n",
    "    \n",
    "    edited_list = []\n",
    "    for ann in train_dataset['annotations']:\n",
    "        if ann['image_id'] in idxs:\n",
    "            edited_list.append(ann)\n",
    "    train_dataset['annotations'] = edited_list\n",
    "\n",
    "    edited_list = []\n",
    "    for ann in train_dataset['images']:\n",
    "        if ann['id'] in idxs:\n",
    "            edited_list.append(ann)\n",
    "    train_dataset['images'] = edited_list\n",
    "    \n",
    "    dir = os.path.join(*annotation.split('/')[:-1])\n",
    "    name = annotation.split('/')[-1].split('.')[0] + '_modified.json'\n",
    "    \n",
    "    save_dir = '/' + os.path.join(dir, name)\n",
    "    with open(save_dir, 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(train_dataset, make_file, indent=\"\\t\")\n",
    "\n",
    "    return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "editer(annotation, idxs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d46d5f9d57d9c71a54d5c977667b5545fd583865b6b06a1b757181255a5d28d0"
  },
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "detection"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}